{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Bootcamp Exam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Imports\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import datasets\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's begin with loading the data\n","iris = datasets.load_iris()\n","X = pd.DataFrame(iris.data, columns=iris.feature_names)\n","y = pd.Series(iris.target, name=\"label\")\n","X = (X - X.mean()) / X.std()  # data normalization\n","data = pd.concat([X, y], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Features Examination"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lets examine the features\n","fig, ax = plt.subplots(\n","    ncols=3, nrows=1, figsize=(10, 4), sharey=True\n",")  # share y for easy comparison\n","\n","# plot the three labels speratley\n","for i in range(3):\n","    # plot a boxplot of the features\n","    data.loc[data[\"label\"] == i, X.columns].plot.box(ax=ax[i])\n","    # change x tick labels to have spaces between words instead of default one line label\n","    ax[i].set_xticklabels([i.replace(\" \", \"\\n\") for i in X.columns])\n","    # add titles\n","    ax[i].set_title(\"Label \"\n","                    + str(i)\n","                    + \"  (N = \"\n","                    + str((data.loc[data[\"label\"] == i, X.columns]).shape[0])\n","                    + \")\"\n","                    )\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can see that the petal length and width are relatively low for label 0. Also, we cn see that large sepal width is typical for that label. However, with large variance. \n","\n","\n","## Hyperparameters Optimization and Models Fitting\n","1. We know that there are three clusters (according to the number of labels)\n","2. We have relatively small dataset which is equally distributed among the labels\n","Before applying any model, let's split the dataset to train-validation-test sets.\n","Since we are working with a relatively small dataset, we shall use stratified k-fold. k-fold helps us to overcome\n","the lack of data by repeating the validition process over different parts of the data, and startification\n","will help us maintaing the label ratios.\n","\n","The validation size will be 25% of the training size.\n","In total, we will have 30 datapoints in the test set, and three folds of 90 datapoints of training set and 30 points of validation set.\n","\n","We will apply three models (no preticular meaning for the order):\n","\n","1) SVM, 2) k-nearest neighbors (KNN) and 3) Logistic regression (LR)\n","1) For the SVM classifier, We will use the validation set to optimize\n","the regularization (C) hyperparameter (keeping the exponent power (gamma) in the automatical scaling of sklearn).\n","Since the dataset is small, we will test only gaussian kernel (a well optimized gaussian kernel will always peform better than linear)\n","\n","2) For the KNN classifier, we will optimize the number of neighbors\n","\n","3) For the logistic regression, we will optimize the regularization parameter"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Parameters to optimize\n","\n","# Regularizationtest C values for SVM  of 1e-3 to 1e3\n","Cs = np.logspace(-3, 4, 8)\n","Cs_lr = np.logspace(-2, 4, 7)  # Regularizationtest C values for LR\n","ks = np.arange(2, 30, 2)  # number of neighbors\n","\n","# Since the dataset is balanced and there is no difference in cost of misclassification towards any of the classes, we will use accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model Fitting\n","\n","scores_svc, scores_knn, scores_lr = {}, {}, {}  # cv scores dictonary\n","scores_test = {}  # test scores dictionary\n","y_tests, y_pred_tests = {}, {}  # save tests labels for confusion matrix\n","\n","estimators = {}  # dictionary for estimator\n","confusions = {}  # dictionary confusion matrix\n","\n","niter = 70\n","\n","for iteration in range(niter):\n","    if iteration % 10 == 0:\n","        print(iteration, \"out of\", niter)\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2\n","    )  # Split to training and test set\n","    skf = StratifiedShuffleSplit(n_splits=3, test_size=0.25).split(\n","        X_train, y_train\n","    )  # k-fold (3-fold) which shuffles the data (since the data is sorted by labels).\n","\n","    iteration_score_svc = np.zeros((len(Cs), 3))\n","    iteration_score_knn = np.zeros((len(ks), 3))\n","    iteration_score_lr = np.zeros((len(Cs_lr), 3))\n","\n","    for i, (train_index, val_index) in enumerate(skf):\n","        # SVM loop\n","        for j, c in enumerate(Cs):\n","            clf = SVC(C=c)  # classifier\n","            clf.fit(X_train.iloc[train_index, :],\n","                    y_train.iloc[train_index])  # fit\n","            y_val_pred = clf.predict(X_train.iloc[val_index, :])  # predict\n","            iteration_score_svc[j, i] = accuracy_score(\n","                y_train.iloc[val_index], y_val_pred\n","            )  # calculate accuracy\n","        # LR loop\n","        for j, c in enumerate(Cs_lr):  #\n","            clf = LogisticRegression(C=c)\n","            clf.fit(X_train.iloc[train_index, :], y_train.iloc[train_index])\n","            y_val_pred = clf.predict(X_train.iloc[val_index, :])\n","            iteration_score_lr[j, i] = accuracy_score(\n","                y_train.iloc[val_index], y_val_pred\n","            )\n","        # KNN loop\n","        for j, k in enumerate(ks):\n","            clf = KNeighborsClassifier(k)\n","            clf.fit(X_train.iloc[train_index, :], y_train.iloc[train_index])\n","            y_val_pred = clf.predict(X_train.iloc[val_index, :])\n","            iteration_score_knn[j, i] = accuracy_score(\n","                y_train.iloc[val_index], y_val_pred\n","            )\n","\n","    # save performance (for this exam, we will assume a small std between the folds)\n","    scores_svc[iteration] = iteration_score_svc.mean(axis=1)\n","    scores_knn[iteration] = iteration_score_knn.mean(axis=1)\n","    scores_lr[iteration] = iteration_score_lr.mean(axis=1)\n","\n","    # retrain the models with the best hyperparameters\n","    estimators['knn_' +\n","               str(iteration)] = KNeighborsClassifier(n_neighbors=ks[scores_knn[iteration].argmax()]).fit(X_train, y_train)\n","    estimators['svc_' +\n","               str(iteration)] = SVC(C=Cs[scores_svc[iteration].argmax()]).fit(X_train, y_train)\n","    estimators['lr_' +\n","               str(iteration)] = LogisticRegression(C=Cs_lr[scores_lr[iteration].argmax()]).fit(X_train, y_train)\n","\n","    # predict test set and store confusion matrix\n","    for method in ['knn', 'lr', 'svc']:\n","        y_pred_tests[method+'_'+str(iteration)] = estimators[method+'_' +\n","                                                             str(iteration)].predict(X_test)\n","        confusions[method+'_'+str(iteration)] = confusion_matrix(y_test,\n","                                                                 y_pred_tests[method+'_'+str(iteration)])\n","        # calculate test score\n","        scores_test[method+'_'+str(iteration)] = accuracy_score(\n","            y_test, y_pred_tests[method+'_'+str(iteration)])\n","\n","\n","# save the CV scores according to the hyperparameter being tuned\n","scores_svc = pd.DataFrame(scores_svc, index=Cs)\n","scores_knn = pd.DataFrame(scores_knn, index=ks)\n","scores_lr = pd.DataFrame(scores_lr, index=Cs_lr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let us examine the hyperparameters of the CV\n","\n","fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(10, 3))\n","\n","hyperparameter_name = [\"Regularization\", \"Regularization\", \"Neighbors\"]\n","hyperparameter_score = [scores_svc, scores_lr, scores_knn]\n","hyperparameter = [Cs, Cs_lr, ks]\n","\n","for i in range(3):\n","    ax[i].errorbar(\n","        hyperparameter[i],\n","        hyperparameter_score[i].mean(axis=1),\n","        yerr=hyperparameter_score[i].std(axis=1),\n","    )\n","    ax[i].scatter(\n","        hyperparameter[i][hyperparameter_score[i].mean(axis=1).argmax()],\n","        hyperparameter_score[i].mean(axis=1).max(),\n","        marker=\"*\",\n","        color=\"r\",\n","        s=150,\n","    )\n","    ax[i].set_ylabel(\"CV Accuracy\")\n","    ax[i].set_xlabel(hyperparameter_name[i])\n","    if i < 2:\n","        ax[i].set_xscale(\"log\")\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's examine our test results by plotting our confusion matrix.\n","# To do so, we will sum our saved matrices\n","\n","fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(10, 3))\n","\n","for i, method in enumerate(['lr', 'knn', 'svc']):\n","    # combine to one confusion matrix\n","    cm = np.sum(np.array([confusions[x]\n","                          for x in confusions if method in x]), axis=0)\n","    # normalize confusion matrix (to percentage)\n","    cm = 100 * cm / np.sum(cm)\n","    # calculate mean and std of scores\n","    scores_method = [scores_test[x] for x in scores_test if method in x]\n","    scores_mean, scores_std = np.round(\n","        [np.mean(scores_method), np.std(scores_method)], 3)\n","\n","    # plot matrix\n","    sns.heatmap(cm, cmap='YlGnBu', annot=True, cbar=False, ax=ax[i])\n","    ax[i].set_title(method + ' ' + str(scores_mean) + 'Â±' + str(scores_std))\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can see that the three classifiers performence is essntially the same.\n","Since we used simple algorithms, this means that the data is easily separable with the given features.\n","We can further look on the coefficients of the logistic regression to see if any of the features\n","is specifically more important for the classification according to its weight.\n","The coefficient shape is 4(number of features) x 3 (number of classes). We will normalize\n","the features ratios for each model and see which are more important to determine each class."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# combine coefficients\n","label0 = {}\n","label1 = {}\n","label2 = {}\n","\n","for iteration in range(niter):\n","    coeff = np.abs(estimators['lr_'+str(iteration)].coef_)\n","    coeff_max = coeff.max(axis=1)\n","    coeff_norm = (coeff.transpose()/coeff_max).transpose()\n","    label0[iteration] = coeff_norm[0]\n","    label1[iteration] = coeff_norm[1]\n","    label2[iteration] = coeff_norm[2]\n","\n","label0 = pd.DataFrame(label0)\n","label1 = pd.DataFrame(label1)\n","label2 = pd.DataFrame(label2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's plot the importance for each label\n","plt.barh(y=np.arange(X.columns.shape[0])+0.25, width=label0.mean(\n","    axis=1), xerr=label0.std(axis=1), alpha=0.5, height=0.2, label='Label 0')\n","\n","plt.barh(y=range(X.columns.shape[0]), width=label1.mean(\n","    axis=1), xerr=label1.std(axis=1), alpha=0.5, tick_label=X.columns, height=0.2, label='Label 1')\n","\n","plt.barh(y=np.arange(X.columns.shape[0])-0.25, width=label2.mean(\n","    axis=1), xerr=label2.std(axis=1), alpha=0.5, height=0.2, label='Label 1')\n","\n","plt.legend(loc='best')"]},{"cell_type":"markdown","metadata":{},"source":["We can see that\n","1) The sepal length has relatively low meaning for class number three\n","2) The petal features has larger weights than the sepal\n","Other than those, it is hard to reach any conclusion.\n","\n","Thanks for reading"]}],"metadata":{"interpreter":{"hash":"0e463326634068f9ccf119d5bdf6bc98aa167a9388ab283be153f117d9b67032"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
